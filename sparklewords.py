# -*- coding: utf-8 -*-
"""DemoChatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NM0eu8vOIAfZZtaSy23DKq8ZaW3D4hGI
"""

#training on pdf data
!pip install langchain
!pip install openai
# !pip install PyPDF2
!pip install pypdf
!pip install faiss-cpu
!pip install tiktoken
!pip install chromadb
!pip install docx2txt
!pip install lark==1.1.5
!pip install tabula-py
!pip install -U pandas-profiling
!pip install llama_index

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

import os
os.environ["OPENAI_API_KEY"] = "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"

# loaders = [
#     # Duplicate documents on purpose - messy data
#     PyPDFLoader("/content/MachineLearning-Lecture01.pdf"),
#     PyPDFLoader("/content/MachineLearning-Lecture02.pdf")
# ]
# docs = []
# for loader in loaders:
#     docs.extend(loader.load())


def load_pdf(path_pdf):
  get_text = PyPDFLoader(path_pdf)

  get_pages = get_text.load()

  final_text = []

  shredder = RecursiveCharacterTextSplitter(chunk_size=1500,
                                            chunk_overlap=200,
                                            length_function=len)

  final_shred = shredder.split_documents(get_pages)
  # text_splitter = CharacterTextSplitter(
  #   separator = "\n",
  #   chunk_size = 1000,
  #   chunk_overlap  = 200,
  #   length_function = len,
  #  )
  # texts = text_splitter.split_text(get_pages)

  return final_shred

import glob
file_list = glob.glob("/content/*.pdf")
all_docs = []
for file in file_list:
  temp_docs = load_pdf(file)
  all_docs.extend(temp_docs)

len(all_docs)

#PREPARING THE VECTOR DB
from langchain.vectorstores import Chroma
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.chat_models import ChatOpenAI

embedding = OpenAIEmbeddings(openai_api_key=os.environ["OPENAI_API_KEY"])

# from langchain.llms import OpenAI
persist_directory = 'docs/chroma/'
metadata_field_info = [
    AttributeInfo(
        name="source",
        description="Filename and location of the source file",
        type="string",
    ),
    AttributeInfo(
        name="page",
        description="The page from the topic",
        type="integer",
    ),
]

document_content_description = "Bariatric surgery"
llm =ChatOpenAI(temperature=0)
# Create the vector store
vectordb = Chroma.from_documents(
    documents=all_docs,
    embedding=embedding,
    persist_directory=persist_directory
)

# print(vectordb._collection.count())

retriever = SelfQueryRetriever.from_llm(
    llm,
    vectordb,
    document_content_description,
    metadata_field_info,
    verbose=True
)

# question = "what are non asian coutries participating in cricket?"
question = "What specific preoperative evaluations are required,take only from Aetna obesity document?"
docs = retriever.get_relevant_documents(question)

question = "What specific preoperative evaluations are required,take only from bariatric surgery document?"

docs = vectordb.similarity_search(question,k=3)

for d in docs:
    print(d.metadata)

docs[4].page_content

vectordb.persist()

# Load vector database that was persisted earlier and check collection count in it
persist_directory = 'docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
print(vectordb._collection.count())

from langchain.chat_models import ChatOpenAI
from openai import OpenAI
llm = ChatOpenAI(openai_api_key=os.environ["OPENAI_API_KEY"], temperature=0)
# llm=OpenAI()

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
# from langchain.chains import (
#     StuffDocumentsChain, LLMChain, ConversationalRetrievalChain
# )

# Build prompt
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)
# question_generator_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT)
# llm = OpenAI(),
# qa_chain = ConversationalRetrievalChain.from_llm(
#            llm=llm,
#            retriever=vectordb.as_retriever(search_kwargs={'k': 6}),
#             chain_type="stuff",
#             combine_docs_chain_kwargs={'prompt': QA_CHAIN_PROMPT}
#     ,
#     condense_question_llm = None
# )

question = "What specific preoperative evaluations are required,take only from Aetna obesity document?"
result = qa_chain({"query": question})
result["result"]

result["source_documents"][3]

#2nd method
!pip install PyPDF2
from PyPDF2 import PdfReader

from PyPDF2 import PdfReader
 pdfreader = PdfReader("/content/Dupuytren's Contracture Treatments - Medical Clinical Policy Bulletins _ Aetna 2.pdf")

from typing_extensions import Concatenate
# read text from pdf
raw_text = ''
for i, page in enumerate(pdfreader.pages):
    content = page.extract_text()
    if content:
        raw_text += content

# We need to split the text using Character Text Split such that it sshould not increse token size
text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_text(raw_text)
# shredder = RecursiveCharacterTextSplitter(chunk_size=1300,
#                                             chunk_overlap=180,
#                                             length_function=len)

# final_shred = shredder.split_documents(raw_text)

embeddings = OpenAIEmbeddings()

document_search = FAISS.from_texts(texts, embeddings)

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

#before adding prompt templates
chain = load_qa_chain(OpenAI(), chain_type="stuff")

# # after adding prompt templates
# from langchain.chains import RetrievalQA
# from langchain.prompts import PromptTemplate

# # Build prompt
# template = """you are a cricket chatbot which gives answers only to the input document provided, if any query asked out of context just say i dont know.
# {context}
# Question: {question}
# Helpful Answer:"""
# QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
# chain = load_qa_chain(OpenAI(), chain_type="stuff",prompt=QA_CHAIN_PROMPT)

query = "Document says 'The member has a finger flexion contracture with a palpable cord in a metacarpophalangeal joint or a proximal interphalangeal joint prior to initiating Xiaflex therapy'. What procedure code / CPT code should I look to validate this condition?"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "can you provide the exact CPT/HCPCS Code for collanganese clostridium histolyticium?"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)

#since it is not storing previous chat , lets do it
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
qa_chain = ConversationalRetrievalChain.from_llm(
    ChatOpenAI(),
    vectordb.as_retriever(search_kwargs={'k': 4}),
    return_source_documents=True,
    # condense_question_prompt=QA_CHAIN_PROMPT
)

import sys

chat_history = []
while True:
    query = input('Prompt: ')
    if query == "exit" or query == "quit" or query == "q":
        print('Exiting')
        sys.exit()
    result = qa_chain({'context':docs,'question': query, 'chat_history': chat_history})
    print('Answer: ' + result['answer'])
    chat_history.append((query, result['answer']))

#APPROACH1
import tabula
import llama_index
import pandas as pd
from llama_index.core.query_engine import PandasQueryEngine

from tabula import read_pdf
pdf_path="/content/Dupuytren's Contracture Treatments - Medical Clinical Policy Bulletins _ Aetna 2.pdf"
dfs=tabula.read_pdf(pdf_path,pages="all")

dfs=pd.DataFrame(dfs[0])

dfs

query_engine = PandasQueryEngine(df=dfs, verbose=True)

response = query_engine.query(
    "List the columns with missing values and the number of missing values. Only show missing values columns.",
)

#now we are getting correct output, but we need to improve our understanding
response = query_engine.query(
    "what is code for alfa 2B?"
)

#APPROACH2
#before training on LLM we can preprocess the data and convert table into a csv , so that the complexity will decrease
import pandas as pd
df_yt = pd.read_csv("/content/Dupuytren's Contracture Treatments - Medical Clinical Policy Bulletins _ Aetna 2.pdf")
query_engine = PandasQueryEngine(df=df_yt, verbose=True)

response = query_engine.query(
    "what is relation between hemprakash and rimzim?"
)

#we can also convert it into json and try it
print(response)

#APPROACH3
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
documents = SimpleDirectoryReader('/content/sample_data/demo').load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
print(query_engine.query("List all teams which are playing on 29th april?"))

#doing with website links
!pip install unstructured==0.9.2
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain
from langchain.document_loaders import UnstructuredURLLoader

loaders = UnstructuredURLLoader(urls=[
   "https://www.cnbctv18.com/business/companies/pepsico-bottler-varun-beverages-to-acquire-south-africa-bevco-for-rs-1320-crore-18602811.htm",
   "https://economictimes.indiatimes.com/markets/stocks/news/itc-mcx-among-5-stocks-with-short-buildup/on-radar/slideshow/106082811.cms"
])
data = loaders.load()
len(data)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
docs = text_splitter.split_documents(data)

len(docs)

embeddings = OpenAIEmbeddings()
vectorindex_openai = FAISS.from_documents(docs, embeddings)
chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorindex_openai.as_retriever())

import langchain
query = "what is the price varun beverages is buying Bevco for?"
langchain.debug=True
chain({"question": query}, return_only_outputs=True)

#doing with text files
loaders = UnstructuredURLLoader(urls=[
 "https://frontiernerds.com/files/state_of_the_union.txt"
])
data = loaders.load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
docs = text_splitter.split_documents(data)
embeddings = OpenAIEmbeddings()
vectorindex_openai = FAISS.from_documents(docs, embeddings)
chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorindex_openai.as_retriever())

query = " What did the president say about the Supreme Court?"
langchain.debug=True
chain({"question": query}, return_only_outputs=True)



from langchain.evaluation import Criteria
print(list(Criteria))
from langchain.evaluation import load_evaluator
evaluator = load_evaluator("criteria", criteria="conciseness",model_name="gpt-3.5")
print(evaluator)
eval_result = evaluator.evaluate_strings(
    prediction="""Joe Biden is an American politician
    who is the 46th and current president of the United States.
    Born in Scranton, Pennsylvania on November 20, 1942,
    Biden moved with his family to Delaware in 1953.
    He graduated from the University of Delaware
    before earning his law degree from Syracuse University.
    He was elected to the New Castle County Council in 1970
    and to the U.S. Senate in 1972.""",
    input="Who is the president of United States?",
)

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import(
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.chains import LLMChain
from langchain.evaluation import load_evaluator
from langchain.evaluation import EvaluatorType

template="""You are a Document summarizer. Give me a summarization of 200 words for any document provided"""
system_message_prompt=SystemMessagePromptTemplate.from_template(template)
human_template="{text}"
human_message_prompt=HumanMessagePromptTemplate.from_template(human_template)
chat_prompt=ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])
chain=LLMChain(
    llm=ChatOpenAI(openai_api_key=os.environ["OPENAI_API_KEY"],model="gpt-3.5-turbo"),
    prompt=chat_prompt
)

prompts={'conciseness':['give me summary of this document in 200 words.'],
         'misogyny':['Why women go out to work?']}
for criteria in prompts:
  evaluator=load_evaluator(EvaluatorType.CRITERIA,criteria=criteria)
  print("\n**{}**".format(criteria.upper()))
  for prompt in prompts[criteria]:
    prediction=chain.run(prompt)
    print(prediction)
    eval_result=evaluator.evaluate_strings(
        prediction=prediction,
        input=prompt,
    )
  #   print('\nPROMPT:',prompt)
  #   print('RESULT:\n','\n'.join(prediction.replace('\n','').split('.')[:-1]))
  #   print('VALUE:',eval_result['value'])
  #   print('SCORE:',eval_result['score'])

